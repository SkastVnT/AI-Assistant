# ğŸ¨ Edit Image Service

> AI-powered image editing service with instruction-based editing, similar to Grok Edit Image.
> **Version: 0.4.0** | Self-hosted, no content filtering

## âœ¨ Features

### Core Generation
- **Text-to-Image**: Generate images from text descriptions
- **Image-to-Image**: Transform images while preserving structure
- **InstructPix2Pix**: Edit images with natural language instructions
- **Inpainting**: Fill in or modify parts of images
- **ControlNet**: Guided generation (Canny, OpenPose, Depth, Lineart, Scribble)

### ğŸ†• v0.4.0 Features (Phase 6)
- **PuLID**: ByteDance NeurIPS 2024 - Identity preservation with Lightning T2I
- **EcomID**: Alibaba IdentityNet - E-commerce identity generation
- **Batch Processing**: Priority queue, async jobs, bulk operations
- **Multi-GPU**: Load balancing across multiple GPUs
- **Model Offloading**: Smart memory optimization, sequential CPU offload

### v0.3.0 Features
- **IP-Adapter**: Image prompts, style transfer, FaceID Plus
- **InstantID**: Zero-shot face swap with InsightFace + ControlNet
- **Inpaint Anything**: SAM + LaMa for click-to-remove objects
- **Smart Edit**: LLM-enhanced editing with web search enrichment
- **Qwen-Image-Edit**: 20B SOTA model for semantic editing
- **Step1X-Edit**: Reasoning mode for complex instructions
- **LoRA Training**: In-app training with dataset preparation
- **Anime ControlNet**: lineart_anime, multi-controlnet support

### Anime & Character
- **Anime Models**: Animagine XL, Anything V5 support
- **Character Search**: AniList, MyAnimeList integration
- **Reference Search**: Danbooru, Gelbooru image search
- **Auto-Tagging**: WD14 Tagger, DeepDanbooru for prompt generation

### Post-Processing
- **Upscaling**: Real-ESRGAN (4x, anime-optimized)
- **Face Restoration**: GFPGAN for face enhancement
- **Full Enhancement Pipeline**: Upscale + face restore combo

## ğŸ–¥ï¸ Web UI Tabs (18 tabs)

| Tab | Description |
|-----|-------------|
| ğŸ“ Text to Image | Generate from text prompt |
| ğŸ–¼ï¸ Image to Image | Transform existing images |
| âœï¸ Edit Image | InstructPix2Pix editing |
| ğŸ¨ Inpaint | Fill in regions with mask |
| ğŸ›ï¸ ControlNet | Pose/edge guided generation |
| ğŸŒ Anime | Specialized anime generation |
| ğŸ¨ IP-Adapter | Image prompt & style transfer |
| ğŸ‘¤ InstantID | Zero-shot face swap |
| âœ‚ï¸ Inpaint Anything | Click-to-remove objects |
| ğŸ§  Smart Edit | LLM-enhanced editing |
| ğŸ” Search | Character & reference search |
| ğŸ·ï¸ Tagger | Auto-tag images for prompts |
| â¬†ï¸ Upscale | Image enhancement |
| ğŸ­ PuLID | **NEW** Identity preservation |
| ğŸ›ï¸ EcomID | **NEW** E-commerce identity |
| ğŸ“¦ Batch | **NEW** Batch processing |
| ğŸ–¥ï¸ GPU & Memory | **NEW** Resource management |
| âš™ï¸ Settings | System info & cache |

## ğŸ› ï¸ Tech Stack

- **Backend**: FastAPI + Python 3.10+
- **AI Models**: 
  - SDXL / FLUX.1 / SD3
  - Step1X-Edit / Qwen-Image-Edit
  - ControlNet, IP-Adapter, InstantID
- **UI**: Gradio / Web Interface
- **Inference**: PyTorch + Diffusers

## ğŸ“¦ Installation

### Prerequisites

- Python 3.10+
- CUDA 11.8+ (for GPU acceleration)
- 8GB+ VRAM (12GB+ recommended)

### Setup

```bash
# Clone repository (if not already)
cd services/edit-image

# Create virtual environment
python -m venv venv

# Activate (Windows)
.\venv\Scripts\activate

# Activate (Linux/Mac)
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### Download Models

Models Ä‘Æ°á»£c táº£i vá» local sáº½ **NHANH HÆ N Ráº¤T NHIá»€U** so vá»›i qua HuggingFace API.

```bash
# Xem danh sÃ¡ch táº¥t cáº£ models
python download_models.py --list

# Táº£i models thiáº¿t yáº¿u (~22GB) - KHUYáº¾N NGHá»Š
python download_models.py --essential

# Táº£i thÃªm anime models
python download_models.py --category anime

# Táº£i táº¥t cáº£ (~65GB)
python download_models.py --all
```

**Xem chi tiáº¿t táº¡i**: [MODELS_DOWNLOAD_LIST.md](MODELS_DOWNLOAD_LIST.md)

## ğŸš€ Usage

### Start Server

```bash
# Windows
.\start.bat

# Linux/Mac
./start.sh

# Or directly
python -m uvicorn app.main:app --host 0.0.0.0 --port 8100 --reload
```

### Web Interface

Open browser: `http://localhost:8100`

### API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| **Generation** |
| POST | `/api/v1/generate` | Text-to-Image generation |
| POST | `/api/v1/edit` | Edit image with text instruction |
| POST | `/api/v1/img2img` | Image-to-image transformation |
| POST | `/api/v1/inpaint` | Inpainting with mask |
| POST | `/api/v1/controlnet` | ControlNet generation |
| **Search** |
| POST | `/api/v1/search/images` | Search reference images |
| POST | `/api/v1/search/character` | Search character info |
| **Tagging** |
| POST | `/api/v1/tag` | Auto-tag image |
| POST | `/api/v1/image-to-prompt` | Convert image to prompt |
| **Upscaling** |
| POST | `/api/v1/upscale` | Upscale image |
| POST | `/api/v1/restore-faces` | Restore faces in image |
| POST | `/api/v1/enhance` | Full enhancement pipeline |
| **System** |
| GET | `/api/v1/models` | List available models |
| GET | `/api/v1/health` | Service health status |
| GET | `/api/v1/vram` | VRAM usage stats |
| POST | `/api/v1/clear-cache` | Clear model cache |

### Example API Call

```python
import requests

response = requests.post(
    "http://localhost:8100/api/v1/edit",
    files={"image": open("input.png", "rb")},
    data={
        "prompt": "Change hair color to blue",
        "model": "sdxl",
        "strength": 0.7
    }
)

# Save result
with open("output.png", "wb") as f:
    f.write(response.content)
```

## ğŸ“ Project Structure

```
edit-image/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py          # Package info (version 0.2.0)
â”‚   â”œâ”€â”€ main.py              # FastAPI app entry point
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ routes.py        # All REST API routes
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py        # Pydantic configuration
â”‚   â”‚   â”œâ”€â”€ pipeline.py      # Diffusion pipeline manager
â”‚   â”‚   â”œâ”€â”€ search.py        # Web search (Danbooru, Gelbooru, AniList, MAL)
â”‚   â”‚   â””â”€â”€ upscaler.py      # Real-ESRGAN, GFPGAN post-processing
â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ gradio_app.py    # Gradio web interface (10 tabs)
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ image_utils.py   # Image processing utilities
â”‚       â”œâ”€â”€ controlnet_utils.py # ControlNet preprocessing
â”‚       â””â”€â”€ tagger.py        # WD14/DeepDanbooru auto-tagging
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.yaml        # Configuration file
â”œâ”€â”€ models/                  # Downloaded model weights
â”œâ”€â”€ outputs/                 # Generated images
â”œâ”€â”€ logs/                    # Application logs
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ Dockerfile              # Docker build
â”œâ”€â”€ docker-compose.yml      # Docker compose
â”œâ”€â”€ start.bat               # Windows startup
â”œâ”€â”€ start.sh                # Linux startup
â”œâ”€â”€ setup.bat               # Windows setup
â””â”€â”€ README.md
```

## âš™ï¸ Configuration

Edit `config/settings.yaml`:

```yaml
server:
  host: "0.0.0.0"
  port: 8100

models:
  default: "sdxl"
  cache_dir: "./models"
  
inference:
  device: "cuda"
  dtype: "float16"
  batch_size: 1
  
controlnet:
  enabled: true
  models:
    - canny
    - openpose
    - depth
```

## ğŸ¯ Supported Models

### Base Models
- SDXL 1.0
- FLUX.1 [dev]
- SD3 Medium
- Animagine XL 3.1

### ControlNet
- Canny Edge
- OpenPose
- Depth
- Lineart
- Segmentation

### Identity Preservation
- InstantID
- PuLID
- IP-Adapter FaceID

## ğŸ“ License

MIT License - Use at your own risk.

## âš ï¸ Disclaimer

This tool is for personal use only. Users are responsible for ensuring compliance with local laws and regulations.
