# Optimized for larger datasets (1500-2000 images)
# Higher capacity, faster training

model:
  pretrained_model_name_or_path: "runwayml/stable-diffusion-v1-5"

lora:
  rank: 32  # Higher rank for more expressiveness
  alpha: 64
  dropout: 0.0  # No dropout needed with large dataset
  target_modules:
    - "to_q"
    - "to_k"
    - "to_v"
    - "to_out.0"
    - "ff.net.0.proj"
    - "ff.net.2"

dataset:
  train_data_dir: "data/train"
  val_data_dir: "data/val"
  resolution: 512
  center_crop: true
  random_flip: true
  caption_extension: ".txt"

training:
  num_train_epochs: 8  # Fewer epochs needed
  train_batch_size: 2  # Larger batch size if GPU allows
  gradient_accumulation_steps: 2
  
  optimizer: "adamw"
  learning_rate: 1.5e-4  # Higher learning rate
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  lr_scheduler: "cosine_with_restarts"
  lr_warmup_steps: 100
  
  validation_epochs: 1
  save_steps: 1
  
  output_dir: "outputs/lora_models"
  checkpoint_dir: "outputs/checkpoints"
  dataloader_num_workers: 8  # More workers for faster data loading
  
  mixed_precision: "fp16"
  gradient_checkpointing: true

logging:
  log_dir: "outputs/logs"
  logging_steps: 20
  use_wandb: false
  generate_samples: true
  sample_steps: 200
  num_samples: 8
  sample_prompts:
    - "a photo of sks style"
    - "sks style landscape"
    - "portrait in sks style"
    - "sks style architecture"
    - "artistic sks style"
    - "sks style sunset"
    - "sks style urban scene"
    - "sks style nature"

advanced:
  noise_offset: 0.1  # Higher noise offset
  snr_gamma: null  # Disable for large dataset
  use_ema: true  # Enable EMA for better quality
  ema_decay: 0.9999
  enable_xformers: true
  train_text_encoder: false
  color_jitter: false
  random_rotation: false
  cache_latents: true  # Cache for faster training
