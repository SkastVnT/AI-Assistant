# Example .env file for train_LoRA_tool
# Copy this to .env and fill in your values

# ====================================
# GROK AI Configuration (NEW in v2.3)
# ====================================

# Get your API key from: https://console.x.ai
GROK_API_KEY=your-grok-api-key-here

# Optional: GROK model to use (default: grok-3)
# GROK_MODEL=grok-3

# ====================================
# HuggingFace Configuration
# ====================================

# HuggingFace token for downloading models
# Get from: https://huggingface.co/settings/tokens
HF_TOKEN=your-huggingface-token-here

# Optional: HuggingFace cache directory
# HF_HOME=./models/huggingface

# ====================================
# WandB Configuration (Optional)
# ====================================

# Weights & Biases for training monitoring
# Get from: https://wandb.ai/authorize
# WANDB_API_KEY=your-wandb-key-here
# WANDB_PROJECT=lora-training
# WANDB_ENTITY=your-username

# ====================================
# Training Paths (Optional)
# ====================================

# Default paths (can override in config files)
# TRAIN_DATA_DIR=./data/train
# OUTPUT_DIR=./output
# MODELS_DIR=./models

# ====================================
# Advanced Settings (Optional)
# ====================================

# CUDA settings
# CUDA_VISIBLE_DEVICES=0
# PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# Mixed precision
# MIXED_PRECISION=fp16

# Number of workers for data loading
# NUM_WORKERS=4
