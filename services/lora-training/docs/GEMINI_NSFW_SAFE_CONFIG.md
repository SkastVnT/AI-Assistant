# Gemini AI Config cho NSFW Training - 100% An to√†n

## üîí V·∫•n ƒë·ªÅ: Gemini kh√¥ng cho ph√©p NSFW

Gemini API t·ª´ ch·ªëi ph√¢n t√≠ch/x·ª≠ l√Ω ·∫£nh NSFW/R18+ content do vi ph·∫°m Terms of Service.

**Nh∆∞ng b·∫°n V·∫™N c√≥ th·ªÉ d√πng Gemini ƒë·ªÉ t·ªëi ∆∞u training!** ‚úÖ

---

## üß† Gi·∫£i ph√°p th√¥ng minh: Metadata-Only Approach

### Workflow an to√†n 100%:

```
1. WD14 Tagger (Local) ‚Üí Ph√¢n t√≠ch ·∫£nh NSFW tr√™n m√°y b·∫°n
2. Extract Metadata ‚Üí Ch·ªâ l·∫•y th·ªëng k√™ (kh√¥ng c√≥ ·∫£nh)
3. Gemini AI ‚Üí Nh·∫≠n metadata, ƒë·ªÅ xu·∫•t config t·ªëi ∆∞u
4. Apply ‚Üí T·ª± ƒë·ªông ƒëi·ªÅn settings v√†o WebUI
```

### Metadata g√¨ ƒë∆∞·ª£c g·ª≠i ƒë·∫øn Gemini?

```json
{
  "total_images": 150,
  "avg_resolution": "768x1024",
  "resolution_stats": {
    "min": "512x512",
    "max": "1024x1536",
    "avg": "768x1024"
  },
  "tag_stats": {
    "total_tags": 2500,
    "unique_tags": 450,
    "avg_tags_per_image": 16.7,
    "most_common": {
      "1girl": 140,
      "solo": 130,
      "anime_style": 150,
      "detailed_background": 80
    }
  },
  "complexity_score": 7.5
}
```

**KH√îNG c√≥:**
- ‚ùå ·∫¢nh NSFW g·ªëc
- ‚ùå T√™n file ·∫£nh
- ‚ùå ƒê∆∞·ªùng d·∫´n folder
- ‚ùå N·ªôi dung ·∫£nh c·ª• th·ªÉ

**CH·ªà c√≥:**
- ‚úÖ S·ªë l∆∞·ª£ng ·∫£nh
- ‚úÖ ƒê·ªô ph√¢n gi·∫£i trung b√¨nh
- ‚úÖ Th·ªëng k√™ tags
- ‚úÖ ƒêi·ªÉm ph·ª©c t·∫°p

---

## üöÄ C√°ch s·ª≠ d·ª•ng

### 1. Setup API Key

```powershell
# Windows PowerShell
$env:GEMINI_API_KEY = "your-api-key-here"

# Ho·∫∑c th√™m v√†o file .env
echo GEMINI_API_KEY=your-api-key > .env
```

### 2. D√πng t·ª´ WebUI (D·ªÖ nh·∫•t!)

1. M·ªü WebUI: http://127.0.0.1:7860
2. Ch·ªçn dataset NSFW c·ªßa b·∫°n
3. (Optional) Click "Auto-Tag with WD14" ƒë·ªÉ tag ·∫£nh local
4. Click **"ü§ñ Get AI-Powered Config (Gemini)"**
5. Ch·ªçn training goal:
   - üéØ High Quality - Ch·∫•t l∆∞·ª£ng cao nh·∫•t
   - ‚öñÔ∏è Balanced - C√¢n b·∫±ng speed/quality
   - ‚ö° Fast - Nhanh, ch·∫•t l∆∞·ª£ng OK
6. Wait ~10s ‚Üí Config t·ª± ƒë·ªông ƒëi·ªÅn!

### 3. D√πng t·ª´ Command Line

```powershell
# Activate venv
.\lora\Scripts\activate.bat

# Ch·∫°y recommender
python utils/config_recommender.py "path/to/your/nsfw/dataset" high_quality

# Output s·∫Ω save v√†o: dataset/recommended_config.json
```

### 4. T√≠ch h·ª£p v√†o Python script

```python
from utils.config_recommender import quick_recommend

# Get recommendations
config = quick_recommend(
    dataset_path="./my_nsfw_dataset",
    training_goal="high_quality"
)

print(f"Recommended LR: {config['learning_rate']}")
print(f"Network Dim: {config['network_dim']}")
print(f"Epochs: {config['epochs']}")
print(f"\nReasoning:\n{config['reasoning']}")
```

---

## üìä V√≠ d·ª• output t·ª´ Gemini

### Input metadata (150 ·∫£nh NSFW, 768x1024):

```json
{
  "total_images": 150,
  "avg_resolution": "768x1024",
  "complexity_score": 7.5,
  "tag_stats": {
    "unique_tags": 450,
    "avg_tags_per_image": 16.7
  }
}
```

### Output recommendations:

```json
{
  "learning_rate": 8e-5,
  "batch_size": 4,
  "epochs": 15,
  "network_dim": 48,
  "network_alpha": 24,
  "optimizer": "AdamW8bit",
  "lr_scheduler": "cosine_with_restarts",
  "min_snr_gamma": 5,
  "use_lora_plus": true,
  "lora_plus_lr_ratio": 16,
  "train_resolution": "768x1024",
  "caption_dropout_rate": 0.05,
  
  "reasoning": "Based on your dataset profile:
  
  1. **Dataset Size (150 images)**: Medium size requires careful balancing. 
     - LR: 8e-5 is conservative enough to avoid overfitting
     - Epochs: 15 provides sufficient training time
  
  2. **High Complexity (7.5/10)**: Dataset has diverse content
     - Network Dim: 48 captures variety while staying efficient
     - Alpha: 24 (half of dim) for stable training
  
  3. **High Resolution (768x1024)**: 
     - Batch Size: 4 to fit in 12GB VRAM
     - Training at native resolution preserves quality
  
  4. **Tag Diversity (450 unique tags)**: 
     - Caption dropout 5% prevents overfitting on specific tags
     - Cosine schedule with restarts helps escape local minima
  
  5. **LoRA+**: Enabled with ratio 16 for better text encoder learning
  
  6. **Min-SNR Gamma 5**: Reduces noise impact on high-res training",
  
  "warnings": [
    "Monitor for overfitting after epoch 10 - reduce epochs if needed",
    "If VRAM limited, reduce batch_size to 2 and increase gradient_accumulation",
    "Consider validation split to track generalization"
  ],
  
  "estimated_vram": "10-12GB",
  "estimated_time": "2-3 hours on RTX 3080"
}
```

---

## üîç Gemini ph√¢n t√≠ch g√¨?

### 1. Dataset Size Analysis
- **<50 images**: Small dataset
  - Lower LR (5e-5)
  - More epochs (20+)
  - Higher dim (64-128) to capture detail
  
- **50-200 images**: Medium dataset
  - Moderate LR (1e-4)
  - Medium epochs (10-15)
  - Balanced dim (32-64)
  
- **>200 images**: Large dataset
  - Higher LR (2e-4)
  - Fewer epochs (5-10)
  - Lower dim (16-32) sufficient

### 2. Complexity Scoring
- **Low (0-4)**: Consistent style/content
  - Smaller dim (8-16)
  - Simpler scheduler
  
- **Medium (5-7)**: Varied content
  - Standard dim (32-48)
  - Cosine scheduler
  
- **High (8-10)**: Very diverse
  - Higher dim (64-128)
  - Advanced techniques (LoRA+, Min-SNR)

### 3. Resolution Optimization
- **512x512**: Standard SD resolution
  - Batch size 8-12
  - Full training speed
  
- **768x1024**: High-res portrait
  - Batch size 4-6
  - 1.5x training time
  
- **1024x1024+**: Very high-res
  - Batch size 2-4
  - Consider bucketing

### 4. Tag Distribution
- **Many unique tags**: High diversity
  - Caption dropout 5-10%
  - Prevent overfitting
  
- **Few repeated tags**: Focused content
  - Lower caption dropout (0-5%)
  - Can use higher LR

---

## ‚öôÔ∏è Training Goals

### üéØ High Quality
```python
training_goal="high_quality"
```
- Conservative LR (~0.8x)
- More epochs (~1.5x)
- Higher network dim
- Best for: Character LoRAs, Style transfer, Important projects

### ‚öñÔ∏è Balanced (Default)
```python
training_goal="balanced"
```
- Standard settings
- Good quality/speed ratio
- Recommended for most use cases

### ‚ö° Fast
```python
training_goal="fast"
```
- Higher LR (~1.5x)
- Fewer epochs (~0.5x)
- Lower dim
- Best for: Testing, experimentation, quick iterations

### üß™ Experimental
```python
training_goal="experimental"
```
- Latest techniques (LoRA+, Min-SNR, etc.)
- Cutting-edge optimizers (Prodigy, DAdaptation)
- May be unstable
- Best for: Research, advanced users

---

## üîê Privacy & Security

### 100% Safe for NSFW:
‚úÖ **Local WD14 Tagging**: ·∫¢nh KH√îNG r·ªùi kh·ªèi m√°y b·∫°n  
‚úÖ **Metadata Only**: Gemini ch·ªâ nh·∫≠n s·ªë li·ªáu th·ªëng k√™  
‚úÖ **No Image Upload**: Kh√¥ng c√≥ ·∫£nh n√†o ƒë∆∞·ª£c upload  
‚úÖ **No Filenames**: T√™n file/folder kh√¥ng ƒë∆∞·ª£c g·ª≠i  
‚úÖ **Encrypted API**: Google API d√πng HTTPS  

### Gemini ch·ªâ th·∫•y:
```
"C√≥ 150 ·∫£nh, resolution trung b√¨nh 768x1024, 
c√≥ 450 unique tags, ƒë·ªô ph·ª©c t·∫°p 7.5/10"
```

### Gemini KH√îNG th·∫•y:
- ‚ùå ·∫¢nh g·ªëc c·ªßa b·∫°n
- ‚ùå N·ªôi dung c·ª• th·ªÉ
- ‚ùå T√™n character/style
- ‚ùå B·∫•t k·ª≥ th√¥ng tin nh·∫°y c·∫£m n√†o

---

## üìà Cost Analysis

### Gemini 2.0 Flash Pricing:
- **Free tier**: 1,500 requests/day
- **Input**: $0.075 per 1M tokens (~15,000 configs)
- **Output**: $0.30 per 1M tokens (~3,000 configs)

### Cost per recommendation:
- Input: ~1,000 tokens (metadata + prompt)
- Output: ~500 tokens (JSON config)
- **Total: $0.0002 per recommendation**

### So s√°nh:
- **GPT-4**: $0.01 per recommendation (50x ƒë·∫Øt h∆°n!)
- **Claude**: $0.005 per recommendation (25x ƒë·∫Øt h∆°n!)
- **Gemini Flash**: $0.0002 (R·∫ª nh·∫•t!)

---

## üõ†Ô∏è Troubleshooting

### "GEMINI_API_KEY not found"
```powershell
# Set trong PowerShell
$env:GEMINI_API_KEY = "your-key"

# Ho·∫∑c t·∫°o .env file
echo GEMINI_API_KEY=your-key > .env
```

### "Error calling Gemini API"
- Check internet connection
- Verify API key is correct
- Check quota (1,500 free/day)
- Fallback s·∫Ω d√πng rule-based recommendations

### "Failed to analyze dataset"
- ƒê·∫£m b·∫£o dataset c√≥ ·∫£nh (.jpg, .png, etc.)
- Check permissions (read access)
- WD14 tags kh√¥ng b·∫Øt bu·ªôc (nh∆∞ng t·ªët h∆°n n·∫øu c√≥)

---

## üí° Best Practices

### 1. Tag dataset tr∆∞·ªõc khi get recommendations
```
Click "Auto-Tag with WD14" ‚Üí Wait complete ‚Üí "Get AI Config"
```
Gemini s·∫Ω c√≥ nhi·ªÅu th√¥ng tin h∆°n t·ª´ tag stats!

### 2. Test v·ªõi Balanced tr∆∞·ªõc
Start v·ªõi `balanced` goal ‚Üí N·∫øu k·∫øt qu·∫£ OK ‚Üí Gi·ªØ nguy√™n  
N·∫øu mu·ªën quality cao h∆°n ‚Üí Switch sang `high_quality`

### 3. Monitor v√† adjust
Gemini recommendations l√† starting point t·ªët, nh∆∞ng:
- Monitor validation loss
- Adjust LR n·∫øu loss kh√¥ng gi·∫£m
- Reduce epochs n·∫øu overfit

### 4. Save configs
WebUI t·ª± ƒë·ªông save recommended configs:
```
dataset/recommended_config.json
```
D√πng l·∫°i cho datasets t∆∞∆°ng t·ª±!

---

## üìö Advanced Usage

### Custom metadata analysis:
```python
from utils.config_recommender import DatasetMetadataAnalyzer

analyzer = DatasetMetadataAnalyzer("./dataset")
metadata = analyzer.analyze()

print(f"Complexity: {metadata['complexity_score']}")
print(f"Most common tags: {metadata['tag_stats']['most_common']}")
```

### Manual override:
```python
from utils.config_recommender import GeminiConfigRecommender

recommender = GeminiConfigRecommender()
config = recommender.recommend_config(metadata, "high_quality")

# Override specific settings
config['learning_rate'] = 1e-4  # Your custom LR
config['epochs'] = 20  # Your custom epochs
```

### Batch recommendations:
```python
datasets = ["dataset1", "dataset2", "dataset3"]

for dataset in datasets:
    config = quick_recommend(dataset, "balanced")
    # Save or use config
```

---

## ‚úÖ Checklist s·ª≠ d·ª•ng

- [ ] Setup GEMINI_API_KEY
- [ ] Select NSFW dataset
- [ ] (Optional) Tag v·ªõi WD14 local
- [ ] Click "Get AI Config"
- [ ] Review recommendations trong logs
- [ ] Check warnings n·∫øu c√≥
- [ ] Adjust n·∫øu c·∫ßn (LR, epochs, etc.)
- [ ] Start training!
- [ ] Monitor validation loss
- [ ] Save successful configs cho l·∫ßn sau

---

## üéâ K·∫øt lu·∫≠n

B·∫°n ho√†n to√†n c√≥ th·ªÉ d√πng **Gemini 2.0 Flash** ƒë·ªÉ t·ªëi ∆∞u config cho NSFW training m√† **100% an to√†n**!

### Key Points:
1. ‚úÖ Gemini ch·ªâ nh·∫≠n metadata, KH√îNG nh·∫≠n ·∫£nh NSFW
2. ‚úÖ WD14 Tagger l√†m vi·ªác local, privacy 100%
3. ‚úÖ AI recommendations r·∫•t ch√≠nh x√°c v√† save time
4. ‚úÖ R·∫ª h∆°n GPT-4 t·ªõi 50x
5. ‚úÖ T√≠ch h·ª£p s·∫µn trong WebUI, 1 click l√† xong

**Happy Training!** üöÄ

---

**Created**: 2024-12-01  
**Version**: 1.0.0  
**Author**: AI Assistant  
**License**: MIT
