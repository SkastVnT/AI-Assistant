# ğŸ¤– Phase 3 Complete - RAG Integration with LLM

## âœ… Features Implemented

### 1. **FREE LLM Integration**
- âœ… Google Gemini API (FREE tier)
- âœ… 15 requests/minute limit
- âœ… 1,500 requests/day limit
- âœ… 1M token context window
- âœ… No cost for personal use

### 2. **RAG Engine**
- âœ… Retrieve relevant chunks (semantic search)
- âœ… Generate answers with LLM
- âœ… Citation tracking
- âœ… Source attribution
- âœ… Confidence scoring

### 3. **Dual Mode Interface**
- âœ… **Search Mode**: Find relevant chunks
- âœ… **RAG Q&A Mode**: AI-generated answers
- âœ… Toggle between modes
- âœ… Visual mode indicators

### 4. **Answer Display**
- âœ… Beautiful AI answer card
- âœ… Markdown formatting support
- âœ… Source documents with relevance scores
- âœ… Expandable search results
- âœ… Copy functionality

### 5. **Smart Features**
- âœ… Language detection (Vietnamese/English)
- âœ… Context-aware responses
- âœ… Top-K retrieval control
- âœ… Error handling
- âœ… Status indicators

---

## ğŸš€ How to Use RAG Mode

### 1. Get FREE Gemini API Key

Visit: https://makersuite.google.com/app/apikey

1. Sign in with Google account
2. Click "Create API Key"
3. Copy the key

### 2. Configure API Key

**Option A: Environment Variable (Recommended)**
```bash
# Windows
set GEMINI_API_KEY=your_api_key_here

# Linux/Mac
export GEMINI_API_KEY=your_api_key_here
```

**Option B: .env File**
```bash
# Create or edit .env file
echo GEMINI_API_KEY=your_api_key_here >> .env
```

### 3. Start Server
```bash
python app.py
```

### 4. Use RAG Mode

1. Upload your documents
2. Click **"RAG Q&A"** button (purple mode)
3. Ask questions in natural language
4. Get AI-generated answers with sources!

---

## ğŸ¯ RAG vs Search Mode

### Search Mode (Blue)
- **What it does**: Finds relevant text chunks
- **Output**: List of matching passages with scores
- **Use when**: You want to browse relevant content
- **Example**: "machine learning" â†’ Shows 5 chunks mentioning ML

### RAG Q&A Mode (Purple)
- **What it does**: AI generates comprehensive answer
- **Output**: Natural language answer + sources
- **Use when**: You want a direct answer to a question
- **Example**: "What is machine learning?" â†’ Full explanation

---

## ğŸ“¡ New API Endpoints

### RAG Query
```bash
POST /api/rag/query
Content-Type: application/json

{
  "query": "What is machine learning?",
  "top_k": 5,
  "language": "auto"  // auto, vi, en
}
```

**Response:**
```json
{
  "answer": "Machine learning is a subset of AI...",
  "sources": [
    {
      "name": "ml_intro.pdf",
      "relevance": 0.95,
      "file_type": ".pdf"
    }
  ],
  "retrieved_chunks": 5,
  "model": "gemini-1.5-flash"
}
```

### Check RAG Status
```bash
GET /api/rag/status
```

**Response:**
```json
{
  "available": true,
  "model": "gemini-1.5-flash",
  "message": "RAG ready"
}
```

---

## ğŸ¨ UI Features

### Mode Toggle
- **Blue button**: Search mode
- **Purple button**: RAG Q&A mode
- **Status badge**: Shows current mode info

### RAG Answer Card
- **Gradient background**: Purple/blue
- **Robot icon**: AI indicator
- **Formatted text**: Bold, italic, code blocks
- **Sources section**: Documents used with scores
- **Expandable details**: View source chunks

### Smart Highlighting
- Query terms highlighted in yellow
- Relevance scores color-coded
- Source attribution visible

---

## ğŸ’¡ Example Queries

### Good RAG Questions:
âœ… "What is machine learning?"  
âœ… "How does neural network work?"  
âœ… "Explain the differences between..."  
âœ… "Summarize the main points about..."  
âœ… "What are the applications of..."  

### Search Queries:
âœ… "machine learning algorithms"  
âœ… "neural network architecture"  
âœ… "deep learning examples"  

---

## ğŸ”§ Configuration

### LLM Settings (config.py)

```python
# LLM Provider
LLM_PROVIDER = "gemini"  # Options: gemini, ollama, huggingface

# Gemini Model (FREE)
GEMINI_MODEL = "gemini-1.5-flash"  # Fast and free
# GEMINI_MODEL = "gemini-1.5-pro"  # Better quality, same free tier

# Retrieval
TOP_K_RESULTS = 5  # More context = better answers
SIMILARITY_THRESHOLD = 0.7  # Minimum relevance
```

### Rate Limits (FREE Tier)

| Limit | Value |
|:------|:------|
| Requests per minute | 15 |
| Requests per day | 1,500 |
| Context window | 1M tokens |
| Cost | **$0** |

---

## ğŸ› ï¸ Architecture

### RAG Pipeline

```
User Question
     â†“
[1] Semantic Search
     â†“
Retrieved Chunks (Top-K)
     â†“
[2] Build Context
     â†“
LLM Prompt (Question + Context)
     â†“
[3] Gemini API
     â†“
Generated Answer
     â†“
Display with Sources
```

### Code Structure

```
RAG Services/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ llm_client.py       # Gemini API wrapper
â”‚   â”‚   â”œâ”€â”€ rag_engine.py       # RAG pipeline
â”‚   â”‚   â”œâ”€â”€ vectorstore.py      # Search
â”‚   â”‚   â””â”€â”€ config.py           # Settings
â”‚   â”‚
â”‚   â”œâ”€â”€ static/js/main.js       # Frontend RAG logic
â”‚   â””â”€â”€ templates/index.html    # RAG UI
â”‚
â””â”€â”€ app.py                       # RAG endpoints
```

---

## ğŸ¯ Prompt Engineering

### Our RAG Prompt Template:

```
You are a helpful AI assistant that answers questions 
based on provided documents.

IMPORTANT INSTRUCTIONS:
1. Answer using ONLY information from documents
2. If documents don't contain info, say so clearly
3. Cite sources by mentioning document names
4. Be concise but comprehensive
5. Detect language and respond in same language
6. Use markdown formatting

DOCUMENTS:
[Retrieved chunks here]

USER QUESTION:
[User query here]

ANSWER:
```

### Why This Works:
- âœ… Clear boundaries (only use documents)
- âœ… Source citation requirement
- âœ… Language flexibility
- âœ… Markdown for better formatting
- âœ… Handles missing information gracefully

---

## ğŸš€ Advanced Usage

### Python API Example

```python
import requests

# RAG Query
response = requests.post(
    'http://localhost:5003/api/rag/query',
    json={
        'query': 'What is the main topic?',
        'top_k': 5,
        'language': 'vi'  # Force Vietnamese
    }
)

result = response.json()
print(f"Answer: {result['answer']}")
print(f"Sources: {result['sources']}")
```

### Integrate with Other Services

```python
# In ChatBot service
import requests

def get_rag_answer(question):
    response = requests.post(
        'http://localhost:5003/api/rag/query',
        json={'query': question}
    )
    return response.json()['answer']

# Use in chat
user_question = "What is AI?"
rag_answer = get_rag_answer(user_question)
```

---

## ğŸ› Troubleshooting

### No API Key Error
```
âš ï¸ LLM not configured. Please set GEMINI_API_KEY
```

**Solution:**
1. Get key from https://makersuite.google.com/app/apikey
2. Set environment variable or .env file
3. Restart server

### Rate Limit Exceeded
```
Error: Resource exhausted (quota)
```

**Solution:**
- Wait 1 minute (15 req/min limit)
- Or wait for daily reset (1500 req/day)
- Consider using Ollama for unlimited local LLM

### Poor Answer Quality
**Solutions:**
- Increase `TOP_K_RESULTS` (more context)
- Lower `SIMILARITY_THRESHOLD` (more relevant chunks)
- Upload more relevant documents
- Use more specific questions

---

## ğŸ’° Cost Comparison

| Service | This RAG | OpenAI GPT-4 | Anthropic Claude |
|:--------|:---------|:-------------|:-----------------|
| **Embedding** | $0 (local) | $0.0001/1K | $0.0001/1K |
| **Vector DB** | $0 (local) | $0.40/GB-month | $0.40/GB-month |
| **LLM** | $0 (Gemini) | $0.03/1K tokens | $0.015/1K tokens |
| **Monthly (1000 queries)** | **$0** | ~$50 | ~$25 |
| **Yearly** | **$0** | ~$600 | ~$300 |

**Total Savings: $300-600/year** ğŸ’°

---

## ğŸ‰ Phase 3 Status

**Status**: âœ… **COMPLETE**

**Achievement Unlocked:**
- ğŸ¤– Full RAG pipeline
- ğŸ’¬ AI-generated answers
- ğŸ“š Source citations
- ğŸ†“ 100% FREE models
- ğŸ¨ Beautiful UI integration

**Ready for**: Phase 4 - Advanced Features

---

## ğŸ“š Alternative LLM Options

### If Gemini Quota Exhausted:

**1. Ollama (Local, Unlimited)**
```bash
# Install Ollama
# Then:
ollama pull llama2
ollama pull mistral
```

**2. HuggingFace Inference API (FREE tier)**
```python
# config.py
LLM_PROVIDER = "huggingface"
HF_MODEL = "google/flan-t5-large"
```

**3. Local Models**
```python
# config.py
LLM_PROVIDER = "local"
LOCAL_MODEL = "Qwen2.5-1.5B-Instruct"
```

---

## ğŸ“ Learning Resources

- [Gemini API Docs](https://ai.google.dev/tutorials/python_quickstart)
- [RAG Explained](https://www.pinecone.io/learn/retrieval-augmented-generation/)
- [Prompt Engineering Guide](https://www.promptingguide.ai/)

---

**Version**: 1.0.0 (Phase 3)  
**Port**: 5003  
**Status**: âœ… RAG Complete  
**Next**: ğŸš€ Advanced Features (Phase 4)
